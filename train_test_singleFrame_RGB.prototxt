name: "stateFarmModel"
layer {
  name: "data"
  type: "ImageData"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    # for images in particular horizontal mirroring and random cropping
    # can be done as simple data augmentations.
    # horizontal, vertical flipping or both simultaneously
    mirror: true # 1 = on, 0 = off
    mean_value: 103.939
    mean_value: 116.779
    mean_value: 123.68
    # crop a `crop_size` x `crop_size` patch:
    # - at random during training
    # - from the center during testing
    crop_size: 227
  }
  image_data_param {
    # target file with train image->class mapping
    source: "stateFarm_train.txt"
    # folder with train images
    root_folder: "train/"
    # Every iteration caffe process a batch, in this case the batch_size is the number of images that each batch will contain
    batch_size: 128
    # apparently this values are being ignored
    new_height: 240
    new_width: 320
  }
}
layer {
  name: "data"
  type: "ImageData"
  top: "data"
  top: "label"
  include {
    phase: TEST
    stage: "test-on-test"
  }
  transform_param {
    # for images in particular horizontal mirroring and random cropping
    # can be done as simple data augmentations.
    # horizontal, vertical flipping or both simultaneously
    mirror: true
    mean_value: 103.939
    mean_value: 116.779
    mean_value: 123.68
    # crop a `crop_size` x `crop_size` patch:
    # - at random during training
    # - from the center during testing
    crop_size: 227
  }
  image_data_param {
    # target file with train image->class mapping
    source: "stateFarm_test.txt"
    # folder with train images
    root_folder: "train/"
    # Every iteration caffe process a batch, in this case the batch_size is the number of images that each batch will contain
    batch_size: 128
    # apparently this values are being ignored
    new_height: 240
    new_width: 320
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  # learning rate and decay multipliers for the filters
  param {
    lr_mult: 1
    decay_mult: 1
  }
  # learning rate and decay multipliers for the biases
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96      # learn 96 filters
    kernel_size: 7      # each filter is 7x7
    stride: 2           # step 2 pixels between each filter application
    weight_filler {
      type: "gaussian"  # initialize the filters from a Gaussian
      std: 0.01         # distribution with stdev 0.01 (default mean: 0)
    }
    bias_filler {
      type: "constant"  # initialize the biases to 0.1
      value: 0.1
    }
  }
}
layer {
  # taking max(x, 0) for each input x
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3 # pool over a 3x3 region
    stride: 2      # step 2 pixels (in the bottom blob) between pooling regions
  }
}
layer {
  # The local response normalization layer performs a kind of “lateral inhibition” 
  # by normalizing over local input regions.This refers to the capacity of an 
  # excited neuron to subdue its neighbors.
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5  # the number of channels to sum over (for cross channel LRN) 
    alpha: 0.0001  # the scaling parameter
    beta: 0.75     # the exponent
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  # learning rate and decay multipliers for the filters
  param {
    lr_mult: 1
    decay_mult: 1
  }
  # learning rate and decay multipliers for the biases
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384   # learn 384 filters
    kernel_size: 5    # each filter is 5x5
    # if g > 1, we restrict the connectivity of each filter to a subset of the 
    # input. Specifically, the input and output channels are separated into 
    # g groups, and the ith output group channels will be only connected to 
    # the ith input group channels.
    group: 2            # 2 groups
    stride: 2           # step 2 pixels between each filter application
    weight_filler {
      type: "gaussian"  # initialize the filters from a Gaussian
      std: 0.01         # distribution with stdev 0.01 (default mean: 0)
    }
    bias_filler {
      type: "constant"  # initialize the biases to 0.1
      value: 0.1
    }
  }
}
layer {
  # taking max(x, 0) for each input x
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3  # pool over a 3x3 region
    stride: 2       # step 2 pixels (in the bottom blob) between pooling regions
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5   # the number of channels to sum over (for cross channel LRN) 
    alpha: 0.0001   # the scaling parameter
    beta: 0.75      # the exponent
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    # learning rate and decay multipliers for the filters
    lr_mult: 1
    decay_mult: 1
  }
  param {
    # learning rate and decay multipliers for the biases
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512     # learn 512 filters
    pad: 1              # specifies the number of pixels to (implicitly) add to each side of the input
    kernel_size: 3      # each filter is 3x3
    weight_filler {
      type: "gaussian"  # Initialize each filter from the Gaussian
      std: 0.01         # distribution with stdev 0.01 (default mean: 0)
    }
    bias_filler {
      type: "constant"  # initialize the biases to 0.1
      value: 0.1
    }
  }
}
layer {
  # taking max(x, 0) for each input x
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    # learning rate and decay multipliers for the filters
    lr_mult: 1
    decay_mult: 1
  }
  param {
    # learning rate and decay multipliers for the biases
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512     # learn 512 filter
    pad: 1              # specifies the number of pixels to (implicitly) add to each side of the input
    kernel_size: 3      # each filter is 3x3
    group: 2            # 2 groups
    weight_filler {
      type: "gaussian"  # Initialize each filter from the Gaussian
      std: 0.01         # distribution with stdev 0.01 
    }
    bias_filler {
      type: "constant"  # initialize the biases to 0.1
      value: 0.1
    }
  }
}
layer {
  # taking max(x, 0) for each input x
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    # learning rate and decay multipliers for the filters
    lr_mult: 1
    decay_mult: 1
  }
  param {
    # learning rate and decay multipliers for the biases
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384     # learn 384 filters
    pad: 1              # specifies the number of pixels to (implicitly) add to each side of the input
    kernel_size: 3      # each filter is 3x3
    group: 2            # 2 groups
    weight_filler {
      type: "gaussian"  # initialize the filter from the Gaussian
      std: 0.01         # distribution with stdev 0.01 
    }
    bias_filler {
      type: "constant"  # initialize the biases
      value: 0.1
    }
  }
}
layer {
  # taking max(x, 0) for each input x
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3  # pool over a 3x3 region
    stride: 2       # step 2 pixels (in the bottom blob) between pooling regions
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  # learning rate and decay multipliers for the weights
  param {
    lr_mult: 1
    decay_mult: 1
  }
  # learning rate and decay multipliers for the biases
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096      # output 4096 features
    weight_filler {
      type: "gaussian"  # initialize the filter from the Gaussian
      std: 0.01         # distribution with stdev 0.01 
    }
    bias_filler {
      type: "constant" # initialize the biases to 0.1
      value: 0.1
    }
  }
}
layer {
  # taking max(x, 0) for each input x
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5   # Sets the probability p that any given unit is dropped. 
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  # learning rate and decay multipliers for the weights
  param {
    lr_mult: 1
    decay_mult: 1
  }
  # learning rate and decay multipliers for the biases
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096      # output 4096 features
    weight_filler {
      type: "gaussian"    # initialize the filter from the Gaussian
      std: 0.01           # distribution with stdev 0.01
    }
    bias_filler {
      type: "constant"    # initialize the biases to 0.1
      value: 0.1
    }
  }
}
layer {
  # taking max(x, 0) for each input x
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5    # Sets the probability p that any given unit is dropped. 
  }
}
layer {
  name: "fc8-ucf"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8-ucf"
  # learning rate and decay multipliers for the weights
  param {
    lr_mult: 10
    decay_mult: 1
  }
  # learning rate and decay multipliers for the biases
  param {
    lr_mult: 20
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10        # output 10 values
    weight_filler {
      type: "gaussian"    # initialize the filter from the Gaussian
      std: 0.01           # distribution with stdev 0.01
    }
    bias_filler {
      type: "constant"    # initialize biases with 0.1
      value: 0
    }
  }
}
layer {
  # The softmax loss layer computes the multinomial logistic loss of the 
  # softmax of its inputs. 
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8-ucf"
  bottom: "label"
  top: "loss"
}
layer {
  # Accuracy scores the output as the accuracy of output with respect to target
  # – it is not actually a loss and has no backward step.
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8-ucf"
  bottom: "label"
  top: "accuracy"
}
